cd
ls
clear
cd
git clone https://github.com/techiescamp/kubeadm-scripts
apt install git
sudo apt install git
su
restart
reboot
uname -a
cd
clear
git clone https://github.com/techiescamp/kubeadm-scripts
apt install git
sudo apt install git
git clone https://github.com/techiescamp/kubeadm-scripts
ls
cd kubeadm-scripts/
ls
cd scripts/
ls
./common.sh 
sudo su
ip a
ls
vim master.sh 
sudo apt install vim
ip a
vim master.sh 
./master.sh 
exit
ip a
exit
cd
ls
cd kubeadm-scripts/scripts/
ls
clear
ls
./common.sh 
./master.sh 
./master.sh --ignore-preflight-errors
history
clear
ls
./common.sh 
sudo apt autoremove
clear
ls
./common.sh 
./master.sh 
sudo kubeadm reset
./master.sh 
ls
kubectl apply -f calico.yaml 
ls
cd
ls
cd Do
cd Documents/
ls
cd
cd kubeadm-scripts/scripts/
ls
exit
ip a
cd
clear
cd kubeadm-scripts/scripts/
ls
clear
sudo kubeadm reset
./common.sh 
./master.sh 
kubectl get nodes
kubectl get pods
kubectl get pods -A
minikube version
telnet 10.0.2.15 6443
iptables 
sudo ufw status
systemctl disable firewalld
cat /etc/hosts
vim master.sh 
vim common.sh 
ipa
ip a
vim master.sh 
sudo kubeadm reset
vim master.sh 
./master.sh 
kubectl get nodes
kubectl get pods
kubectl get pods -A
vim master.sh 
kubectl get nods
kubectl get nodes
kubectl get nodes -n kube-system -o wide
kubectl get pods -n kube-system -o wide
kubectl config current-context
cd
pwd
cd .kube/
ls
cat config 
ls
cd
ls
vim nginx.yaml
vim nginx-deploy.yaml
kubectl apply -f nginx-deploy.yaml 
vim nginx-svc.yaml
kubectl apply -f nginx-svc.yaml 
k get pods
kubectl get pods
kubectl get svc
kubectl get nodes
kubectl get nodes -o wide
kubectl get pods
kubectl get pods -o wide
exit
exit
kubectl get pods
history
kubectl get nodes
ssh worker2
ssh itslinux foss
cat /etc/host
cat /etc/hosts
exit
cd
kubectl get nodes
kubectl get pods
kubectl get pods -A
clear
kubectl get nodes
ssh worker2
exit
kubectl get nodes
exit
kubectl get nodes
kubectl get nodes -o wide
kubectl get nodes
telnet 192.168.1.12 2049
ip a
telnet 192.168.1.12 2049
kubectl get nodes
kubectl get pods
ip a
cd
cd kubeadm-scripts/scripts/
ls
vim master.sh 
ls
cd
cd .kube/
ks
ls
cat config 
kubeadm configview
kubeadm config view
kubeadm config -h
kubeadm token list
kubeadm token create
kubeadm token list
kubeadm config view
cd /etc/kubernetes/
ls
cd pki/
ls
cd ..
cd manifests/
ls
cd ..
sudo find /etc/kubernetes/ -name "*.conf"
ls
cat admin.conf 
sudo cat admin.conf 
kubeadm token create
kubeadm token create mopi9x.ao3tpxw5yg0erf3o --print-join-command
kubeadm token list
kubeadm token create slepds.xr8bm3ugaptuy8rg --print-join-command
kubeadm token generate
kubeadm token create h10f55.hxa8d58uaorgbh1l --print-join-command
clear
cd
clear
kubectl get nodes
history
kubeadm token list
kubectl get nodes
kubectl get pods
exit
kubectl get pods
kubectl get nodes
kubectl get pods
kubectl get pods -n rook-ceph
k get pods
kubectl get pods
kubectl get pods -n rook-ceph
k get pods
kubectl get pods
kubectl describe pod nfs-subdir-external-provisioner-78c459c4df-4xc96
kubectl get pods
kubectl get nodes
kubectl get pods
kubectl get pods -n rook-ceph
kubectl delete -f crds.yaml -f common.yaml -f operator.yaml
cd
cd rook/deploy/examples/
kubectl delete -f crds.yaml -f common.yaml -f operator.yaml
kubectl delete -f cluster.yaml 
exit
kubectl get pods -n rook-ceph
kubectl get pods 
kubectl get pods -n rook-ceph
kubectl get pods -n rook-ceph --watch
kubectl get pods
kubectl get pods -n rook-ceph
exit
kubectl get pods
kubectl get nodes
kubectl get pods -n rook-ceph
kubectl delete -f cluster.yaml 
kubectl delete -f crds.yaml -f common.yaml -f operator.yaml
kubectl get pods -n rook-ceph
k delete pod csi-cephfsplugin-dwtg4 -n rook-ceph
kubectl delete pod csi-cephfsplugin-dwtg4 -n rook-ceph
kubectl get pods -n rook-ceph
kubectl delete -f crds.yaml -f common.yaml -f operator.yaml
ls
cd
cd rook/deploy/examples/
ls
kubectl delete -f crds.yaml -f common.yaml -f operator.yaml
kubectl delete -f cluster.yaml 
clear
kubectl get pods -n rook-ceph
kubectl create -f crds.yaml -f common.yaml -f operator.yaml
kubectl get pods -n rook-ceph
clear
kubectl -n rook-ceph patch cephclusters.ceph.rook.io rook-ceph -p '{"metadata":{"finalizers": []}}' --type=merge
kubectl get pods -n rook-ceph
kubectl -n rook-ceph delete cephcluster rook-ceph
kubectl get pods -n rook-ceph
kubectl delete -f operator.yaml
kubectl delete -f common.yaml
kubectl get pods -n rook-ceph
kubectl get ns
kubectl delete ns rook-ceph
kubectl get pods -n rook-ceph
kubectl -n rook-ceph patch cephclusters.ceph.rook.io rook-ceph -p '{"metadata":{"finalizers": []}}' --type=merge
kubectl get pods -n rook-ceph
k describe pod csi-cephfsplugin-dwtg4 -n rook-ceph
kubectl describe pod csi-cephfsplugin-dwtg4 -n rook-ceph
clear
kubectl get pods -n rook-ceph
k delete pod rook-ceph-operator-844f855666-k2ws4 -n rook-ceph
kubectl delete pod rook-ceph-operator-844f855666-k2ws4 -n rook-ceph
kubectl get pods -n rook-ceph
kubectl delete -f ../wordpress.yaml
kubectl delete -f ../mysql.yaml
kubectl delete -n rook-ceph cephblockpool replicapool
kubectl delete storageclass rook-ceph-block
kubectl delete -f kube-registry.yaml
kubectl -n rook-ceph delete cephcluster rook-ceph
kubectl -n rook-ceph get cephcluster
kubectl delete -f operator.yaml
kubectl delete -f common.yaml
alias k=kubectl
kubectl get pods -n rook-ceph
kubectl -n rook-ceph patch cephclusters.ceph.rook.io rook-ceph -p '{"metadata":{"finalizers": []}}' --type=merge
kubectl get pods -n rook-ceph
k delete ns rook-ceph
kubectl get pods -n rook-ceph
kubectl get pods -n rook-ceph --watch
kubectl get pods -n rook-ceph
k get ns
k get ns --watch
k get ns
kubectl get pods -n rook-ceph
k get ns
k delete ns rook-ceph
k get ns
kubectl -n rook-ceph patch cephclusters.ceph.rook.io rook-ceph -p '{"metadata":{"finalizers": []}}' --type=merge
k get ns
kubectl -n rook-ceph patch cephclusters.ceph.rook.io rook-ceph -p '{"metadata":{"finalizers": []}}' --type=merge
k get ns
kubectl create -f crds.yaml -f common.yaml -f operator.yaml
k get ns
kubectl get pods -n rook-ceph
k get ns
kubectl api-resources --verbs=list --namespaced -o name   | xargs -n 1 kubectl get --show-kind --ignore-not-found -n rook-ceph-system
k get ns
kubectl -n rook-ceph patch cephclusters.ceph.rook.io rook-ceph -p '{"metadata":{"finalizers": []}}' --type=merge
k get cm
k get cm -n rook-ceph
k get all -n rook-ceph
k delete cm rook-ceph-mon-endpoints -n rook-ceph
kubectl api-resources --verbs=list --namespaced -o name   | xargs -n 1 kubectl get --show-kind --ignore-not-found -n rook-ceph
k delete secret rook-ceph-mon -n rook-ceph
k delete cm rook-ceph-mon-endpoints -n rook-ceph
kubectl get namespace rook-ceph -o json > rook-ceph.json
kubectl replace --raw "/api/v1/namespaces/rook-ceph/finalize" -f ./rook-ceph.json
kubectl get namespace
k get all -n rook-ceph
k get cm -n rook-ceph
k edit cm rook-ceph-mon-endpoints -n rook-ceph
k get secret -n rook-ceph
k edit secret rook-ceph-mon -n rook-ceph
k delete secret rook-ceph-mon -n rook-ceph
k edit secret rook-ceph-mon -n rook-ceph
clear
k get secret -n rook-ceph
k edit secret rook-ceph-mon -n rook-ceph
kubectl -n rook-ceph patch cephclusters.ceph.rook.io rook-ceph -p '{"metadata":{"finalizers": [ceph.rook.io/disaster-protection]}}' --type=merge
kubectl -n rook-ceph patch ceph.rook.io/disaster-protection rook-ceph -p '{"metadata":{"finalizers": []}}' --type=merge
kubectl -n rook-ceph patch ceph.rook.io/disaster-protection -p '{"metadata":{"finalizers": []}}' --type=merge
kubectl -n rook-ceph patch ceph.rook.io/disaster-protection rook-ceph -p '{"metadata":{"finalizers": []}}' --type=merge
kubectl get namespace
kubectl get namespace rook-ceph -o json > rook-ceph.json
ls
cat rook-ceph.json 
k get ns
kubectl -n rook-ceph patch cephclusters.ceph.rook.io/disaster-protection -p '{"metadata":{"finalizers": []}}' --type=merge
kubectl -n rook-ceph patch secrets rook-ceph-mon --type merge -p '{"metadata":{"finalizers": []}}'
k get ns
k get secrets -n rook-ceph
k get cm -n rook-ceph
k delete cm rook-ceph-mon-endpoints -n rook-ceph
kubectl -n rook-ceph patch configmap rook-ceph-mon-endpoints --type merge -p '{"metadata":{"finalizers": []}}'
k get ns
clear
k get pods
kubectl create -f crds.yaml -f common.yaml -f operator.yaml
k get pods
k get pods -n rook-ceph
ls
k create -f cluster-test.yaml 
k create -f toolbox.yaml 
k get pods -n rook-ceph
k describe pod csi-cephfsplugin-provisioner-7f7874c95d-gb99p -n rook-ceph
k get nodes
clear
k get pods -n rook-ceph
k describe pod rook-ceph-osd-prepare-worker2-2l5s4 -n rook-ceph
clear
k get pods -n rook-ceph
kubectl -n rook-ceph exec -it deploy/rook-ceph-tools -- bash
kubectl -n rook-ceph exec -it rook-ceph-tools-68b98695bb-g2cmv -- bash
kubectl -n rook-ceph exec -it rook-ceph-tools-68b98695bb-g2cmv bash
k exec -it rook-ceph-tools-68b98695bb-g2cmv -n rook-ceph bash
k exec -it rook-ceph-tools-68b98695bb-g2cmv -n rook-ceph sh
k exec pod -it rook-ceph-tools-68b98695bb-g2cmv -n rook-ceph sh
k exec -it rook-ceph-tools-68b98695bb-g2cmv -n rook-ceph bash
k exec -it rook-ceph-tools-68b98695bb-g2cmv -n rook-ceph -- bash
k get pods -n rook-ceph
k exec -it rook-ceph-tools-68b98695bb-g2cmv -n rook-ceph -- bash
k get deploy -n rook-ceph
k exec -it rook-ceph-tools-68b98695bb-g2cmv -n rook-ceph -- sh
k exec -it rook-ceph-tools-68b98695bb-g2cmv -n rook-ceph  sh
kubectl -n rook-ceph rollout status deploy/rook-ceph-tools
kube -- bash
k exec -it rook-ceph-tools-68b98695bb-g2cmv -n rook-ceph -- bash
k get pods -n rook-ceph
k exec -it rook-ceph-tools-68b98695bb-g2cmv -n rook-ceph sh
k exec -it rook-ceph-tools-68b98695bb-g2cmv -n rook-ceph --sh
k exec -it rook-ceph-tools-68b98695bb-g2cmv -n rook-ceph --bash
k exec -it rook-ceph-tools-68b98695bb-g2cmv -n rook-ceph -- bash
k exec -it rook-ceph-tools-68b98695bb-g2cmv -n rook-ceph -- sh
k get nodes -o wide
k exec -it rook-ceph-tools-68b98695bb-g2cmv -n rook-cephsh
k exec -it rook-ceph-tools-68b98695bb-g2cmv -n rook-ceph sh
k exec -it rook-ceph-tools-68b98695bb-g2cmv -n rook-ceph bash
k exec -it rook-ceph-tools-68b98695bb-g2cmv -n rook-ceph -- bash
k exec --help
k exec -it rook-ceph-tools-68b98695bb-g2cmv -n rook-ceph sh
k get pods -n rook-ceph
k get nodes -o wide
ssh worker 2
ssh worker2
kubectl -n rook-ceph exec -it deploy/rook-ceph-tools -- bash
k get deploy -n rook-ceph
k describe pod rook-ceph-tools-68b98695bb-g2cmv -n rook-ceph
clear
k get pods -n rook-ceph
k get pods 
k get pods -n rook-ceph
k get nodes -o wide
cd /etc/systemd/system/
ls
cd kubelet.service.d/
ls
vim 10-kubeadm.conf 
ip a
exit
ip a
history
clear
kubectl api-resources --verbs=list --namespaced -o name   | xargs -n 1 kubectl get --show-kind --ignore-not-found -n rook-ceph
kubectl -n rook-ceph patch cephcluster rook-ceph --type merge -p '{"spec":{"cleanupPolicy":{"confirmation":"yes-really-destroy-data"}}}'
kubectl api-resources --verbs=list --namespaced -o name   | xargs -n 1 kubectl get --show-kind --ignore-not-found -n rook-ceph
kubectl -n rook-ceph patch configmap rook-ceph-mon-endpoints --type merge -p '{"metadata":{"finalizers": []}}'
kubectl -n rook-ceph patch secrets rook-ceph-mon --type merge -p '{"metadata":{"finalizers": []}}'
kubectl api-resources --verbs=list --namespaced -o name   | xargs -n 1 kubectl get --show-kind --ignore-not-found -n rook-ceph
for CRD in $(kubectl get crd -n rook-ceph | awk '/ceph.rook.io/ {print $1}'); do     kubectl get -n rook-ceph "$CRD" -o name |     xargs -I {} kubectl patch -n rook-ceph {} --type merge -p '{"metadata":{"finalizers": []}}'; done
kubectl api-resources --verbs=list --namespaced -o name   | xargs -n 1 kubectl get --show-kind --ignore-not-found -n rook-ceph
clear
k get pods -n rook-ceph
kubectl get pods -n rook-ceph
for CRD in $(kubectl get crd -n rook-ceph | awk '/ceph.rook.io/ {print $1}'); do     kubectl get -n rook-ceph "$CRD" -o name |     xargs -I {} kubectl patch -n rook-ceph {} --type merge -p '{"metadata":{"finalizers": []}}'; done
kubectl -n rook-ceph patch configmap rook-ceph-mon-endpoints --type merge -p '{"metadata":{"finalizers": []}}'
kubectl -n rook-ceph patch secrets rook-ceph-mon --type merge -p '{"metadata":{"finalizers": []}}'
kubectl api-resources --verbs=list --namespaced -o name   | xargs -n 1 kubectl get --show-kind --ignore-not-found -n rook-ceph
kubectl get pods -n rook-ceph
alias k=kubectl
k exec -it rook-ceph-tools-68b98695bb-4jdcf -n rook-ceph -- bash
k exec -it rook-ceph-tools-68b98695bb-4jdcf -n rook-ceph -- bash --v=10
k exec -it rook-ceph-tools-68b98695bb-4jdcf -n rook-ceph -- bash --v=100
k exec -it rook-ceph-tools-68b98695bb-4jdcf -n rook-ceph -- bash --v=5
k logs rook-ceph-tools-68b98695bb-4jdcf -n rook-ceph
kubectl get pods -n rook-ceph -o wide
sudo systemctl status systemd-resolved.service
k exec -it rook-ceph-tools-68b98695bb-4jdcf -n rook-ceph -- bash -v=5
k exec -it rook-ceph-tools-68b98695bb-4jdcf -n rook-ceph -- bash -v=10
kubectl get pods -n rook-ceph -o wide
clear
k get nodes -o wide
k logs rook-ceph-tools-68b98695bb-4jdcf -n rook-ceph
k logs rook-ceph-tools-68b98695bb-4jdcf -n rook-ceph -v=10
clear
k get nodes -o wide
cd /etc/systemd/system/
ls
cd kubelet.service.d/
ls
vim 10-kubeadm.conf 
sudo vim 10-kubeadm.conf 
systemctl daemon-reload
systemctl restart kubelet
k get nodes -o wide
k get pods
k get pods -n rook-ceph
k exec -it rook-ceph-tools-68b98695bb-s5zcc -n rook-ceph -- bash
vim 10-kubeadm.conf 
k get pods -n rook-ceph
k get nodes -o wide
ssh worker2
vim 10-kubeadm.conf 
ssh worker2
k get nodes -o wide
k get pods
k get pods -o wide
k exec -it nfs-subdir-external-provisioner-78c459c4df-zncqb -- bash
k get pods -n rook-ceph
k get pods -n rook-ceph -o wide
k get pods -o wide
k get nodes -o wide
k describe node worker2
k get nodes -o wide
vim 10-kubeadm.conf 
k get nodes -o wide
cat /etc/hosts
k get nodes -o wide
vim 10-kubeadm.conf 
k get nodes
k get nodes -o wide
k get pods
k get pods -o wide
k get nodes -o wide
systemctl status kubelet
k get nodes -o wide
k get pods
k get nodes -o wide
k get pods -n rook -ceph -o wide
k get pods -n rook-ceph -o wide
k exec -it rook-ceph-tools-68b98695bb-p22s5 -- bash
k exec -it rook-ceph-tools-68b98695bb-p22s5 -n rook-ceph -- bash
ls
vim 10-kubeadm.conf 
k get nodes -o wide
k get pods
k run nginx --image=nginx
k get pods
k delete pod nginx
k exec -it rook-ceph-tools-68b98695bb-p22s5 -n rook-ceph -- bash
k get pods -n rook-ceph
k exec -it rook-ceph-tools-68b98695bb-2hcsz -n rook-ceph bash
k exec -it rook-ceph-tools-68b98695bb-2hcsz -n rook-ceph -- bash
k get pods
k exec -it nginx-deployment-57d84f57dc-8qd5q -- bash
k get pods -o wide
k get pods -n kube-system
k logs kube-apiserver-test -n kube-system --since10min
k logs kube-apiserver-test -n kube-system --since 10m
k get nodes -o wide
cd
cd kubeadm-scripts/scripts/
vim master.sh 
cd /etc/systemd/system/kubelet.service.d/
vim 10-kubeadm.conf 
k get nodes -o wide
k get pods
k describe pod nfs-subdir-external-provisioner-78c459c4df-fv8r7 
clear
k get pods
k get pods -n rook-ceph
k get pods -n rook-ceph -o wide
cd 
cd rook/
ls
cd deploy/examples/
vim cluster-test.yaml 
ls
vim cluster.yaml 
k delete -f cluster-test.yaml 
k apply -f cluster.yaml 
k get pods -n rook-ceph
k get pods
k get pods -n rook-ceph
kubectl delete -n rook-ceph cephblockpool replicapool
kubectl delete storageclass rook-ceph-block
kubectl delete -f csi/cephfs/kube-registry.yaml
kubectl -n rook-ceph delete cephcluster rook-ceph
kubectl -n rook-ceph get cephcluster
kubectl delete -f operator.yaml
kubectl delete -f common.yaml
kubectl delete -f crds.yaml
k get ns
clear
vim cluster.yaml 
kubectl create -f crds.yaml -f common.yaml -f operator.yaml
kubectl create -f cluster.yaml
k create -f toolbox.yaml 
k get pods -n rook-ceph
k exec -it rook-ceph-tools-68b98695bb-xm5ld -n rook-ceph -- bash
k get pods -n rook-ceph
k get pods -n rook-ceph -o wide
$ kubectl -n rook-ceph logs rook-ceph-osd-fl8fs
...
kubectl -n rook-ceph logs rook-ceph-osd-fl8fs
k get pods -n rook-ceph -o wide
vim cluster.yaml 
k apply -f cluster.yaml 
k get pods -n rook-ceph -o wide
k get pods -n rook-ceph 
k get pods -n rook-ceph --watch
k get pods -n rook-ceph 
lsblk
vim cluster.yaml 
k apply -f cluster.yaml 
k get pods -n rook-ceph 
cd /dev/
ls
which sda
k get pods -n rook-ceph 
vim cluster.yaml 
cd
cd rook/deploy/examples/
ls
vim cluster.yaml 
k apply -f cluster.yaml 
k get pods -n rook-ceph 
vim cluster.yaml 
k get pods -n rook-ceph 
vim cluster.yaml 
k get pods -n rook-ceph 
k apply -f cluster.yaml 
lsblk
k get pods -n rook-ceph 
ssh worker2
k get nodes -o wide
ssh worker2
k get pods
kubectl -n rook-ceph patch cephcluster rook-ceph --type merge -p '{"spec":{"cleanupPolicy":{"confirmation":"yes-really-destroy-data"}}}'
kubectl delete -f operator.yaml
kubectl delete -f common.yaml
kubectl delete -f crds.yaml
exit
kubectl get nodes -o wide
kubectl get svc
ls
kubectl get svc
kubectl get all
kubectl get svc nginx2
alias k=kubectl
k get pods
clear
kubectl edit configmap -n kube-system kube-proxy
k get pods
cd
ls
kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.13.9/config/manifests/metallb-native.yaml
k get ns
k get pods -n metallb
k get pods -n metallb-system
k get pods -n metallb-system -o wide
k get pods -n metallb-system
vim metallb-conf.yaml
vim metallb-pool.yaml
k apply -f metallb-pool.yaml 
k get ipaddresspool.metallb.io -n metallb-system
k describe ipaddresspool.metallb.io first-pool -n metallb-system
ls
vim metallb-layer2_conf.yaml
k apply -f metallb-layer2_conf.yaml 
k get l2advertisement.metallb.io -A
k describe l2advertisement.metallb.io example -n metallb-system
k get all -n metall-system
k get all -n metallb-system
k get deploy
k create deploy nginx2 --image=nginx
k get deploy
k expose deploy nginx2 --port 80 --type LoadBalancer
k get svc
curl 192.168.1.240
clear
k get pods -n metallb-system
cat metallb-pool.yaml 
cat metallb-layer2_conf.yaml 
clear
k get pod nginx2
k get deploy nginx2
kubectl get svc nginx2
curl 192.168.1.240
clear
exit
velero backup logs staging-backup
clear
velero backup logs staging-backup
velero backup describe staging-backup
velero backup logs staging-backup
velero backup describe staging-backup
velero backup logs staging-backup
alias k=kubectl
k get pods
clear
k get pods
clear
wget https://github.com/vmware-tanzu/velero/releases/tag/v1.10.3
ls
sudo mv v1.10.3 /usr/local/bin
ls
cd
velereo version
velero version
cd /usr/local/bin/
ls
rm -r v1.10.3 
sudo rm -r v1.10.3 
ls
cd
wget https://github.com/vmware-tanzu/velero/releases/download/v1.10.3/velero-v1.10.3-linux-amd64.tar.gz
tar -xvf velero-v1.10.3-linux-amd64.tar.gz -C /tmp
sudo mv /tmp/velero-v1.10.3-linux-amd64/velero /usr/local/bin
velero version
aws s3 mb s3://velero-test-hpe-bucket --region us-west-2
sudo apt  install awscli
aws s3 mb s3://velero-test-hpe-bucket --region us-west-2
cd /tmp/
ls
cd /usr/local/bin/
ls
vim velero 
./velero install         --use-volume-snapshots=false         --no-default-backup-location         --no-secret         --plugins velero/velero-plugin-for-aws:v1.1.0 -n spp-velero
kubectl logs deployment/velero -n spp-velero
kubectl describe deployment velero -n spp-velero | grep Image
ls
kubectl get ns
kubectl get all -n velero
kubectl get all -n spp-velero
kubectl create namespace staging
k get ns
k create deploy nginx-deploy --image=nginx -n staging 
k get deploy -n staging
k get pods -n staging
velero backup create staging-backup --include-namespaces staging
spp-velero backup create staging-backup --include-namespaces staging
velero backup create staging-backup --include-namespaces staging
velero backup create staging-backup --include-namespaces spp-velero
velero backup create staging-backup --include-namespaces staging
velero backup create -h
velero backup create staging-backup --include-namespaces staging
velero backup create backup-test
kubectl delete namespace/spp-velero clusterrolebinding/velero
kubectl delete crds -l component=velero  
ls
./velero install         --use-volume-snapshots=false         --no-default-backup-location         --no-secret         --plugins velero/velero-plugin-for-aws:v1.1.0 -n velero
kubectl logs deployment/velero -n velero
kubectl describe deployment velero -n spp-velero | grep Image
kubectl describe deployment velero -n velero | grep Image
k get ns
velero backup create staging-backup --include-namespaces staging
velero backup describe staging-backup
velero backup-location -h
velero backup-location --help
velero backup-location set -h
clear
ls
kubectl delete namespace/velero clusterrolebinding/velero
kubectl delete crds -l component=velero  
k get ns
cd
ls
vim velero-credentials.yaml
velero install     --provider aws     --plugins velero/velero-plugin-for-aws:v1.0.1     --bucket vlasis-cluster-backup     --backup-location-config region=ap-south-1     --snapshot-location-config region=ap-south-1     --secret-file ./velero-credentials
ls
rm -r velero-credentials.yaml 
ls
vim velero-credentials.txt
velero install     --provider aws     --plugins velero/velero-plugin-for-aws:v1.0.1     --bucket vlasis-cluster-backup     --backup-location-config region=ap-south-1     --snapshot-location-config region=ap-south-1     --secret-file ./velero-credentials
ls
vim velero-credentials
rm -r velero-credentials.txt 
vim velero-credentials
ls
velero install     --provider aws     --plugins velero/velero-plugin-for-aws:v1.0.1     --bucket vlasis-cluster-backup     --backup-location-config region=ap-south-1     --snapshot-location-config region=ap-south-1     --secret-file ./velero-credentials
kubectl logs deployment/velero -n velero
k get ns
velero backup create staging-backup --include-namespaces staging
velero backup logs staging-backup
velero backup describe staging-backup
velero backup logs staging-backup
velero backup describe staging-backup
velero backup logs staging-backup
aws configure -h
aws configure --help
aws configure 
velero backup logs staging-backup
aws iam create-access-key --user-name velero
aws configure 
aws iam create-access-key --user-name velero
ls
exit
kubectl get nodes
kubectl get nodes -o wdie
kubectl get nodes -o wide
kubeadm token create
kubeadm token list
kubectl get nodes -o wide
kubectl get pods -o wide
kubectl get pods
kubectl get pods -o wide
kubectl describe pod nfs-subdir-external-provisioner-78c459c4df-bgp4p
kubectl get pods -o wide
kubectl describe pod nfs-subdir-external-provisioner-78c459c4df-bgp4p
exit
cd
ls
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml
ls
kubectl top pods
kubectl top nodes
kubectl get apiservices | grep metrics
kubectl edit deployments.apps -n kube-system metrics-server
kubectl get apiservices | grep metrics
kubectl top nodes
kubectl get apiservices | grep metrics
kubectl edit deployments.apps -n kube-system metrics-server
kubectl get apiservices | grep metrics
kubectl edit deployments.apps -n kube-system metrics-server
kubectl get apiservices | grep metrics
kubectl apply -f components.yaml
kubectl get apiservices | grep metrics
k delete service/metrics-server -n  kube-system
alias k=kubectl
k delete service/metrics-server -n  kube-system
k delete deployment.apps/metrics-server  -n  kube-system
k delete apiservices.apiregistration.k8s.io v1beta1.metrics.k8s.io
k delete clusterroles.rbac.authorization.k8s.io system:aggregated-metrics-reader
kubectl get apiservices | grep metrics
curl -LO https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml
ls
vim components.yaml 
k apply -f components.yaml 
vim components.yaml 
kubectl get pods -n kube-system
kubectl top nodes
k apply -f components.yaml 
vim components.yaml 
k apply -f components.yaml 
k top nodes
k top pods
kubectl get --raw /api/v1/nodes/docker-desktop/proxy/metrics/resource
ls
kubectl delete service/metrics-server -n  kube-system
kubectl delete deployment.apps/metrics-server  -n  kube-system
kubectl delete apiservices.apiregistration.k8s.io v1beta1.metrics.k8s.io
kubectl delete clusterroles.rbac.authorization.k8s.io system:aggregated-metrics-reader
kubectl delete clusterroles.rbac.authorization.k8s.io system:metrics-server 
kubectl delete clusterrolebinding metrics-server:system:auth-delegator
kubectl delete clusterrolebinding system:metrics-server          
kubectl delete rolebinding metrics-server-auth-reader -n kube-system 
kubectl delete serviceaccount metrics-server -n kube-system
ls
cat components.yaml 
rm -r components.yaml 
clear
ls
curl -LO https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml
ls
vim components.yaml 
k apply -f components.yaml 
k top pods
kubectl get pods -n kube-system
k describe pod metrics-server-f94d65847-5zlns -n kube-system
clear
vim components.yaml 
k apply -f components.yaml 
k top nodes
k top pods
exit
ls
cd
ls
kubectl get pods
kubectl get pods -o wide
cd /etc/kubernetes/
ls
cat kubelet.conf 
cat admin.conf 
sudo su
cd
cd .kube/
ls
vim config 
exit
cd
ls
alias k=kubectl
k get pods
clear
ls
k get pods
k get svc
k describe svc nginx2
kubectl get pv
kubectl get pvc
kubectl get pvc -n velero
kubectl get pv -n velero
kubectl get ns
kubectl describe ns velero
kubectl api-resources --verbs=list --namespaced -o name   | xargs -n 1 kubectl get --show-kind --ignore-not-found -n velero
kubectl api-resources --verbs=list --namespaced -o name | xargs -n 1 kubectl get --show-kind --ignore-not-found -n velero
kubectl describe pod velero-d58f7db9b-5l4s4 
kubectl get pods -n velero
kubectl get pods -n velero -o wide
alias k=kubectl
k get ns
k delete ns staging
k get ns
k get role
k get role -n velero
k get rolebinding -n velero
k get clusterrolebinding -n velero
kubectl delete crds -l component=velero
k get ns
k delete ns staging
cd
k get ns
kubectl delete crds -l component=velero
ls
cd
ls
alias k=kubect
alias k=kubectl
k get secret
k get secret -n velero
k get secret -n staging
k get svc
k get svc -n velero
gcloud config list
sudo apt install google-cloud-cli
sudo snap install google-cloud-cli
clear
alias k=kubectl
k get pods
k get ns
k get all -n velero
k delete ns velero
k get all -n velero
k delete pod/velero-d58f7db9b-5l4s4 -n velero
k delete ns velero
velero -n velero uninstall velero
kubectl delete namespace/velero clusterrolebinding/velero
k get ns
k get pods
k get pods -n velero
kubectl delete namespace/velero clusterrolebinding/velero
k get pods -n velero
k get pods -n velero-o wide
k get pods -n velero -o wide
k get pods -n velero
k get ns
kubectl delete namespace/velero clusterrolebinding/velero
kubectl delete crds -l component=velero
k get ns
k describe ns velero
k get ns
kubectl api-resources --verbs=list --namespaced -o name   | xargs -n 1 kubectl get --show-kind --ignore-not-found -n <namespace>
kubectl api-resources --verbs=list --namespaced -o name   | xargs -n 1 kubectl get --show-kind --ignore-not-found -n velero
kubectl api-resources --verbs=list --namespaced -o name   | xargs -n 1 kubectl get --show-kind --ignore-not-found -n staging
kubectl get namespace "stucked-namespace" -o json   | tr -d "\n" | sed "s/\"finalizers\": \[[^]]\+\]/\"finalizers\": []/"   | kubectl replace --raw /api/v1/namespaces/velero/finalize -f -
kubectl get namespace "velero" -o json   | tr -d "\n" | sed "s/\"finalizers\": \[[^]]\+\]/\"finalizers\": []/"   | kubectl replace --raw /api/v1/namespaces/velero/finalize -f -
k get ns
kubectl get namespace "staging" -o json   | tr -d "\n" | sed "s/\"finalizers\": \[[^]]\+\]/\"finalizers\": []/"   | kubectl replace --raw /api/v1/namespaces/staging/finalize -f -
k get ns
clear
cd
velero version
ls
vim velero-credentials 
vim credentials-velero 
ls
rm -r velero-credentials 
ls
velero install     --provider gcp     --bucket $BUCKET     --secret-file ./credentials-velero
velero install     --provider gcp     --bucket kubernetes_project_bucket     --secret-file ./credentials-velero
velero install     --provider gcp --plugins velero/velero-plugin-for-gcp     --bucket kubernetes_project_bucket     --secret-file ./credentials-velero
k get ns
k get all -n velero
k get ns
k get pod -n staging
k run nginx --image=nginx -n staging
k get pod
k get pod -n stagin
k get pod -n staging
clear
k get pod staging
clear
k get pod -n staging
velero backup create staging-backup --include-namespaces staging
velero backup describe staging-backup
velero backup logs staging-backup
velero backup describe staging-backup
velero backup logs staging-backup
kubectl delete namespace staging
k get ns
velero restore create --from-backup staging-backup
k get ns
velero restore describe staging-backup-20230522162938
velero restore logs staging-backup-20230522162938
velero restore get
ls
vim credentials-velero 
kubectl delete namespace/velero clusterrolebinding/velero
kubectl delete crds -l component=velero
k get ns
velero install --provider gcp --plugins velero/velero-plugin-for-gcp --bucket kubernetes_project_bucket --backup-location-config region=us-west --snapshot-location-config region=us-west --secret-file ./credentials-velero
k get ns
k get all -n velero
k run nginx --image=nginx -n velero
k delete pod nginx -n velero
k run nginx --image=nginx -n staging
k get pod -n staging
velero backup create staging-backup --include-namespaces staging
velero backup describe staging-backup
velero backup logs staging-backup
kubectl delete namespace/velero clusterrolebinding/velero
kubectl delete crds -l component=velero
k get ns
velero install --provider gcp --plugins velero/velero-plugin-for-gcp:v1.6.0 --bucket kubernetes_project_bucket --backup-location-config region=us-west --snapshot-location-config region=us-west --secret-file ./credentials-velero
k get all -n velero
k get ns
k get all -n velero
velero backup create staging-backup --include-namespaces staging
velero backup describe staging-backup
kubectl delete namespace/velero clusterrolebinding/velero
kubectl delete crds -l component=velero
velero version
ls
vim credentials-velero 
k get all -n velero
k get ns
k get all -n velero
k get ns
k get pod -n staging
velero backup create staging-backup --include-namespaces staging
velero backup describe staging-backup
velero backup logs staging-backup
velero backup create staging-backup --include-namespaces staging
velero backup describe staging-backup
velero backup logs staging-backup
kubectl delete namespace/velero clusterrolebinding/velero
kubectl delete crds -l component=velero
k get ns
exit
cd /etc/kubernetes/pki/
s
ls
cat ca.crt 
ls
cat ca.key 
sudo su
exit
cd /etc/hosts
cd /etc
cd host
cat /etc/hosts
sudo vim /etc/hosts
exit
kubectl get nodes
kubectl get pods
df -h
cd
clear
alias k=kubectl
k get nodes
k describe node test
kubectl version
kubectl get no -o 'go-template={{range .items}}{{$taints:=""}}{{range .spec.taints}}{{if eq .effect "NoSchedule"}}{{$taints = print $taints .key ","}}{{end}}{{end}}{{if not $taints}}{{.metadata.name}}{{ "\n"}}{{end}}{{end}}'
k get nodes
k describe node worker2
k describe node worker1
clear
k describe node test
k describe node worker1
k describe node worker2
kubectl describe node <nodename> | grep Taints
kubectl describe node test | grep Taints
kubectl describe node worker1 | grep Taints
kubectl describe node worker2 | grep Taints
k get pods
k get pods -n osticket
k get pods -n nextcloud
k describe pod nextcloud-deployment-854d948588-dxnpz -n nextcloud
clear
cd /var/lib/kubelet/
sudo su
clear
cd /var/snap/
ls
cd snap-store/
ls
cd common/
ls
cd ..
k get pods -n nextcloud
k get pods
k get pods -n osticket
k get pods -n osticket -o wide
exit
alias k=kubectl
df -h
k describe node test
k taint test  node-role.kubernetes.io/control-plane:NoSchedule-
k taint node test node-role.kubernetes.io/control-plane:NoSchedule-
k get pods
k get pods -o wide
k delete pod rocket-chat-deployment-6c8c65655-8hlfk
k delete pod rocket-chat-deployment-6c8c65655-ckjqk
clear
cd
alias k=kubectl
k get pods
k get pods -o wide
k taint node test node-role.kubernetes.io/control-plane:NoSchedule-
vim /etc/exports
ls /sbin/mount.nfs
kssh worker1
ssh worker1
k get nodes
k get nodes -o wide
ssh 192.168.1.8
ssh 192.168.1.12
telnet 192.168.1.12:2661
telnet 192.168.1.12:2049
telnet 192.168.1.12 2049
nfs logs
sudo mount -t
mkdir data
ls
cd data/
cd ..
sudo mount -t nfs 192.168.1.12:/srv/data /data
sudo apt install nfs-common
sudo mount -t nfs 192.168.1.12:/srv/data /data
df -h
exit
df -h
clear
alias k=kubectl
k get pdos
k get pods
k get pods -o wide
cd
ls
velero version
vim credentials-velero 
$ velero install     --provider aws     --plugins velero/velero-plugin-for-aws:v1.0.1     --bucket velero-test-hpe-bucket     --backup-location-config region=us-west-2     --snapshot-location-config region=us-west-2     --secret-file ./velero-credentials
$ velero install     --provider aws     --plugins velero/velero-plugin-for-aws:v1.0.1     --bucket kubernetes-velero-bucket     --backup-location-config region=us-west-2     --snapshot-location-config region=us-west-2     --secret-file ./credentials-velero
velero install     --provider aws     --plugins velero/velero-plugin-for-aws:v1.0.1     --bucket kubernetes-velero-bucket     --backup-location-config region=us-west-2     --snapshot-location-config region=us-west-2     --secret-file ./credentials-velero
$ kubectl get all -n velero
kubectl get all -n velero
k get ns
kubectl get all -n velero -o wide
k get ns
k get pods -n staging
k run nginx --image=nginx -n staging
k get pods -n staging
velero backup create staging-backup --include-namespaces staging
velero backup describe staging-backup
velero backup logs staging-backup
velero backup create staging-backup --include-namespaces staging
velero backup describe staging-backup
velero backup logs staging-backup
velero backup describe staging-backup
k get ns
k get pods -n staging
velero backup create staging-backup --include-namespaces staging
kubectl delete backup staging-backup -n staging
velero backup create staging-backup --include-namespaces staging
velero backup delete staging-backup
k get pods
k get pods -n staging
velero backup describe staging-backup
kubectl delete backup staging-backup -n staging
velero backup describe staging-backup
velero get backup velero-staging
velero get backup staging-backup
velero backup delete staging-backup --force
velero get backup staging-backup
k describe velero backup staging-backup
k describe velero backup staging-backup -n velero
velero get backup staging-backup
df -h
velero backup create staging2-backup --include-namespaces staging
velero backup describe staging2-backup
velero backup logs staging2-backup
velero get backup staging-backup
velero get backup staging2-backup
velero get backup staging-backup
velero get backup staging2-backup
velero get backup staging-backup
k delete backup staging2-backup -n staging
k delete backup staging2-backup -n velero
velero get backup staging-backup
velero get backup staging2-backup
velero backup delete staging2-backup
velero get backup staging2-backup
velero get backup staging-backup
k delete backup staging-backup -n velero
velero get backup staging-backup
velero backup delete staging-backup
velero get backup staging-backup
clear
velero backup create staging-backup --include-namespaces staging
velero backup describe staging-backup
velero backup logs staging-backup` for more information)


velero backup logs staging-backup
velero backup logs staging-backup` for more information)


velero backup describe staging-backup
velero get backup staging-backup
k delete backupstaging-backup -n velero
k delete backup staging-backup -n velero
velero backup delete staging-backup
velero backup create staging-backup --include-namespaces staging
velero backup describe staging-backup
velero backup logs staging-backup
aws configure
aws
velero backup describe staging-backup
velero get backup staging-backup
k delete backup staging-backup -n velero
velero backup delete staging-backup
velero backup create staging-backup --include-namespaces staging
velero backup describe staging-backup
velero backup logs staging-backup
velero backup describe staging-backup
`velero backup logs staging-backup`
ls
$ velero install     --provider aws     --plugins velero/velero-plugin-for-aws:v1.0.1     --bucket velero-test-hpe-bucket     --backup-location-config region=us-west-2     --snapshot-location-config region=us-west-2     --secret-file ./velero-credentials
kubectl delete namespace/velero clusterrolebinding/velero
kubectl delete crds -l component=velero
$ velero install     --provider aws     --plugins velero/velero-plugin-for-aws:v1.0.1     --bucket velero-test-hpe-bucket     --backup-location-config region=us-west-2     --snapshot-location-config region=us-west-2     --secret-file ./velero-credentials
$ velero install     --provider aws     --plugins velero/velero-plugin-for-aws:v1.0.1     --bucket kubernetes-velero-bucket     --backup-location-config region=us-west-2     --snapshot-location-config region=us-west-2     --secret-file ./credentials-velero
velero install     --provider aws     --plugins velero/velero-plugin-for-aws:v1.0.1     --bucket kubernetes-velero-bucket     --backup-location-config region=us-west-2     --snapshot-location-config region=us-west-2     --secret-file ./credentials-velero
k get pods -n velero
kubectl logs deployment/velero -n velero
k get all -n velero
velero get backup staging-backup
velero get backup staging2-backup
velero backup create staging-backup --include-namespaces staging
velero backup describe staging-backup
velero backup logs staging-backup
k get ns
velero backup describe staging-backup
velero get backup staging2-backup
velero get backup staging-backup
k delete backup staging-backup -n velero
velero backup delete staging-backup
velero backup create staging-backup --include-namespaces staging
velero backup describe staging-backup
velero backup logs staging-backup
velero backup describe staging-backup
k delete backup staging-backup -n velero
velero backup delete staging-backup
velero backup create staging-backup --include-namespaces staging
velero backup describe staging-backup
velero backup logs staging-backup
velero backup describe staging-backup
k delete backup staging-backup -n velero
velero backup delete staging-backup
velero backup create staging-backup --include-namespaces staging
velero backup logs staging-backup
velero backup describe staging-backup
velero backup logs staging-backup
velero backup describe staging-backup
velero backup logs staging-backup
k delete backup staging-backup -n velero
velero backup delete staging-backup
velero backup create staging-backup --include-namespaces staging
velero backup describe staging-backup
velero backup logs staging-backup
k delete backup staging-backup -n velero
velero backup delete staging-backup
exit
cd
alias k=kubectl
k get pods
k get pods -o wide
history
velero backup describe staging-backup
velero get backup staging-backup
clear
ls
vim credentials-velero 
$ velero install     --provider aws     --plugins velero/velero-plugin-for-aws:v1.6.0     --bucket <your-s3-bucket-name>     --backup-location-config region=<region>     --snapshot-location-config region=<region>     --secret-file ./credentials     --use-restic
velero install     --provider aws     --plugins velero/velero-plugin-for-aws:v1.6.0     --bucket kubernetes-velero-backup     --backup-location-config region=us-west-2     --snapshot-location-config region=us-west-2     --secret-file ./credentials-velero 
k get all -n velero
k get ns
velero backup create staging-backup --include-namespaces staging
velero backup describe staging-backup
velero backup logs staging-backup
ls
vim credentials-velero 
velero backup logs staging-backup
ls
vim credentials-velero 
cat credentials-velero 
k delete backup staging-backup -n velero
velero backup delete staging-backup
velero install     --provider aws     --plugins velero/velero-plugin-for-aws:v1.6.0     --bucket kubernetes-velero-backup     --backup-location-config region=us-west-2     --snapshot-location-config region=us-west-2     --secret-file ./credentials-velero 
k get all -n velero
velero backup create staging-backup --include-namespaces staging
velero backup describe staging-backup
velero backup logs staging-backup
echo $AWS_SECRET_ACCESS_KEY
echo $AWS_ACCESS_KEY_ID
vim credentials-velero 
aws configure
echo $AWS_SECRET_ACCESS_KEY
aws login
ls
vim credentials-velero 
echo $AWS_SECRET_ACCESS_KEY
clear
k get all -n velero
ls
velero version
cd /usr/local/bin/
ls
clear
k delete backup staging-backup -n velero
velero backup delete staging-backup
kubectl delete namespace/velero clusterrolebinding/velero
kubectl delete crds -l component=velero
velero version
ls
clear
vim credentials-velero 
cd
ls
velero install     --provider aws     --plugins velero/velero-plugin-for-aws:v1.6.0     --bucket kubernetes-velero-backup     --backup-location-config region=us-west-2     --snapshot-location-config region=us-west-2     --secret-file ./credentials-velero 
k get all -n velero
k get ns
velero backup create staging-backup --include-namespaces staging
velero backup describe staging-backup
velero backup logs staging-backup
k delete backup staging-backup -n velero
velero backup delete staging-backup
kubectl delete namespace/velero clusterrolebinding/velero
kubectl delete crds -l component=velero
velero install     --provider aws     --plugins velero/velero-plugin-for-aws:v1.6.0     --bucket kubernetes-velero-bucket     --backup-location-config region=us-west-2     --snapshot-location-config region=us-west-2     --secret-file ./credentials-velero 
k get all -n velero
velero backup create staging-backup --include-namespaces staging
velero backup describe staging-backup
velero backup logs staging-backup
velero backup describe staging-backup
k delete backup staging-backup -n velero
velero backup delete staging-backup
kubectl delete namespace/velero clusterrolebinding/velero
kubectl delete crds -l component=velero
velero install     --provider aws     --plugins velero/velero-plugin-for-aws:v1.6.0     --bucket kubernetes-velero-bucket     --backup-location-config region=us-west-2     --snapshot-location-config region=us-west-2     --secret-file ./credentials-velero 
k get all -n velero
velero backup create staging-backup --include-namespaces staging
velero backup describe staging-backup
velero backup logs staging-backup
k get secret -n velero
cat credentials-velero 
kubectl -n velero get deploy velero -ojson | jq .spec.template.spec.containers[0].volumeMounts
kubectl -n velero exec -ti deploy/velero -- bash
k get pods -n velero
k exec -it velero-8479848ddc-bqx6b -n velero -- bash
BSL_SECRET=$(kubectl get backupstoragelocations.velero.io -n velero <bsl-name> -o yaml -o jsonpath={.spec.credential.name})
velero backup logs staging-backup
velero backup describe staging-backup
k get ns
ls
cd /usr/local/bin/
la
vim velero
velero snapshot-location get -o yaml
velero backup logs staging-backup
kubectl -n velero logs deploy/velero
lookup kubernetes-velero-bucket.s3.us-west-2.amazonaws.com on 10.96.0.10:53
k get cm -n veleor
k get cm -n velero
cat /etc/resolv
cat /etc/resolf
cd
cd /etc/
ls
vim resolv.conf 
cd
kubectl -n velero logs deploy/velero > velero.txt
ls
cat velero
cat velero.txt 
cd
clear
exit
velero version
ls
exit
cd
clear
ls
history
ls
cat velero.txt 
clear
ls
rm -r velero.txt 
k delete backup staging-backup -n velero
alias k=kubectl
k delete backup staging-backup -n velero
velero backup delete staging-backup
kubectl delete namespace/velero clusterrolebinding/velero
kubectl delete crds -l component=velero
velero install     --provider aws     --plugins velero/velero-plugin-for-aws:v1.6.0     --bucket kubernetes-velero-bucket     --backup-location-config region=us-west-2     --snapshot-location-config region=us-west-2     --secret-file ./credentials-velero 
k get all -n velero
velero backup create staging-backup --include-namespaces staging
velero backup describe staging-backup
velero backup logs staging-backup
k delete backup staging-backup -n velero
velero backup delete staging-backup
velero backup create staging-backup --include-namespaces staging
velero backup describe staging-backup
velero backup logs staging-backup
k get deploy -n velero
history
kubectl -n velero logs deploy/velero
k delete backup staging-backup -n velero
velero backup delete staging-backup
kubectl delete namespace/velero clusterrolebinding/velero
kubectl delete crds -l component=velero
velero install     --provider aws     --plugins velero/velero-plugin-for-aws:v1.6.0     --bucket kubernetes-velero-bucket     --backup-location-config region=us-west-2     --snapshot-location-config region=us-west-2     --secret-file ./credentials-velero --use-restic
cd /tmp/
ls
cd
ls
cat components.yaml 
clear
ls
rm -r velero-v1.10.3-linux-amd64.tar.gz 
ls
cd /usr/local/bin/
ls
cd
k delete backup staging-backup -n velero
velero backup delete staging-backup
kubectl delete namespace/velero clusterrolebinding/velero
k get all -n veleor
k get all -n velero
kubectl delete crds -l component=velero
k get pods
cd /usr/local/bin/
ls
rm -r velero 
sudo rm -r velero 
ls
cd
ls
clear
wget https://github.com/vmware-tanzu/velero/releases/download/v1.11.0/velero-v1.11.0-linux-amd64.tar.gz
ls
tar -xvf velero-v1.11.0-linux-amd64.tar.gz -C /tmp
sudo mv /tmp/velero-v1.11.0-linux-amd64/velero /usr/local/bin
velero version
$ velero install     --provider aws     --plugins velero/velero-plugin-for-aws:v1.0.1     --bucket velero-test-hpe-bucket     --backup-location-config region=us-west-2     --snapshot-location-config region=us-west-2     --secret-file ./velero-credentials
velero install     --provider aws     --plugins velero/velero-plugin-for-aws:v1.0.1     --bucket kubernetes-velero-bucket     --backup-location-config region=us-west-2     --snapshot-location-config region=us-west-2     --secret-file ./credentials-velero
k get all -n velero
velero backup create staging-backup --include-namespaces staging
velero backup describe staging-backup
velero backup logs staging-backup
k get pods -n staging
ls
cat credentials-velero 
exit
cd
ls
cat credentials-velero 
ping google
k get pods
kubectl get pods
kubectl get pods -o wide
ping google.com
cd
clear
alias k=kubectl
k get pods
k get pods -n staging
k get pods -n staging -o wide
k get nodes -o wide
k get logs deploy -n velero
kubectl logs deployment/velero -n velero
k get secret -n velero
k describe secret cloud-credentials -n velero
k get pods -n velero
velero install --crds-only --dry-run -o yaml | kubectl apply -f -
kubectl logs deployment/velero -n velero
velero get backup-location
ls
rm -r credentials-velero 
vim velero-credentials
k delete backup staging-backup -n velero
velero delete staging-backup -n velero
velero backup delete staging-backup 
kubectl delete namespace/velero clusterrolebinding/velero
k get all -n velero
k get pods
k get pods -o wide
k get pods -n velero
k get pods -n staging
k get pods -n velero
k get pods -o wide
k get pods -
k get pods 
$velero install     --provider aws     --plugins velero/velero-plugin-for-aws:v1.0.1     --bucket kubernetes-velero-bucket     --backup-location-config region=us-west-2     --snapshot-location-config region=us-west-2     --secret-file ./velero-credentials
ls
velero install     --provider aws     --plugins velero/velero-plugin-for-aws:v1.0.1     --bucket kubernetes-velero-bucket     --backup-location-config region=us-west-2     --snapshot-location-config region=us-west-2     --secret-file ./velero-credentials
kubectl logs deployment/velero -n velero
k get pods -n velero
kubectl logs deployment/velero -n velero
k get pods -n velero
kubectl logs deployment/velero -n velero
sudo tee -a hosts
sudo vim /etc/hosts
exit
kubectl delete crds -l component=velero
sudo vim /etc/resolv.conf
sudo vim /etc/hosts
k get pods -n kube-system
alias k=kubectl
k get pods -n kube-system
nslookup kubernetes-velero-bucket.s3.us-west-2.amazonaws.com
sudo vim /etc/hosts
nslookup kubernetes-velero-bucket.s3.us-west-2.amazonaws.com
k get pods -n velero
$ velero backup create staging-backup --include-namespaces staging
velero backup create staging-backup --include-namespaces staging
velero backup describe staging-backup
velero backup logs staging-backup
velero backup describe staging-backup
sudo vim /etc/hosts
nslookup kubernetes-velero-bucket.s3.us-west-2.amazonaws.com
history
k logs deployment/velero -n velero
nslookup google.com 10.96.0.10
kubectl get endpoints kube-dns
kubectl exec --namespace=kube-system $(kubectl get pods --namespace=kube-system -l k8s-app=kube-dns -o name) -c kubedns -- nslookup google.com 127.0.0.1
k get pods -n kube-system
kubectl exec --namespace=kube-system $(kubectl get pods --namespace=kube-system -l k8s-app=kube-dns -o name) -c kubedns -- nslookup kubernetes 127.0.0.1
nslookup google.com
kubectl get endpoints kube-dns
kubectl get endpoints kube-dns -n kube-system
kubectl exec --namespace=kube-system $(kubectl get pods --namespace=kube-system -l k8s-app=kube-dns -o name) -c kubedns -- nslookup google.com 127.0.0.1
kubectl get endpoints kube-dns -n kube-system -o wide
kubectl describe pod coredns-5d78c9869d-8sjmn --namespace=kube-system
k get pods -n kube-system
kubectl describe pod coredns-5d78c9869d-8sjmn --namespace=kube-system
k get pods -n kube-system
kubectl describe pod coredns-5d78c9869d-fqz56 --namespace=kube-system
df -h
kubectl describe pod coredns-5d78c9869d-fqz56 --namespace=kube-system
kubectl edit  pod coredns-5d78c9869d-fqz56 --namespace=kube-system
k get pods -n kube-system
k get pods -n kube-system -o wide
iptables
kubectl describe pod coredns-5d78c9869d-fqz56 --namespace=kube-system
k get pods -n calico
k get ns

k get pods -n calico-system
cd
ls
cd kubeadm-scripts/
ls
cd scripts/
ls
cat calico.yaml 
history
kubectl describe pod coredns-5d78c9869d-fqz56 --namespace=kube-system
kubectl exec --namespace=kube-system $(kubectl get pods --namespace=kube-system -l k8s-app=kube-dns -o name) -c kubedns -- nslookup google.com 127.0.0.1
nslookup google.com
ip a
k get all -n kube-system
alias k=kubectl
k get all -n kube-system
k describe pod/calico-node-fqhwr
k get pods -n kube-system
k describe pod calico-node-fkt2b -n kube-system
k get pods -n kube-system
k logs coredns-5d78c9869d-8sjmn -n kube-system
[200~kubectl -n kube-system edit configmap coredns
kubectl -n kube-system edit configmap coredns
kubectl describe clusterrole system:coredns -n kube-system
kubectl get svc --namespace=kube-system
k logs coredns-5d78c9869d-8sjmn -n kube-system
sudo ufw status
k logs coredns-5d78c9869d-8sjmn -n kube-system
vim /etc/network
vim /etc/network/interfaces
cd /etc/
ls
cd network
ls
cd ..
ca networks
cat networks
cd /usr/sbin/
ls
ls | grep iptables
curl https://<kube-apiserver-IP>:6443
k logs coredns-5d78c9869d-8sjmn -n kube-system
k describe pod calico-node-fkt2b -n kube-system
k get pods -n kube-system
k describe pod coredns-5d78c9869d-fqz56 -n kube-system
k get pods -n kube-system
k describe pod coredns-5d78c9869d-fqz56 -n kube-system
history
kubectl exec --namespace=kube-system $(kubectl get pods --namespace=kube-system -l k8s-app=kube-dns -o name) -c kubedns -- nslookup google.com 127.0.0.1
kubectl logs deployment/velero -n velero
vim /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
k get oids -n kube-system
k get pods -n kube-system
k delete pod coredns-5d78c9869d-8sjmn -n kube-system
k delete pod coredns-5d78c9869d-fqz56 -n kube-system
k get pods -n kube-system
k logs coredns-5d78c9869d-kx65k -n kube-system
exit
cd
alias k=kubectl
k get pods
k get pods -n kube-system
k logs coredns-5d78c9869d-mkk2x -n kube-system
k get pods -n kube-system
k logs coredns-5d78c9869d-kx65k -n kube-system
k get pods -n kube-system
k get pods -n kube-system -o wide
k describe pod coredns-5d78c9869d-kx65k -n kube-system
k get pods -n kube-system
k logs coredns-5d78c9869d-mkk2x
k logs coredns-5d78c9869d-mkk2x -n kube-system
k logs coredns-5d78c9869d-kx65k -n kube-system
k get pods -n kube-system
exit
k get pods
alias k=kubectl
k get pods
k get pods -o wide
k get svc
k get pods -o wide
clear
cd
clear
ping 192.168.1.1
vim /etc/hosts
vim /etc/resolv.conf 
k get nodes -o wide
cd kubeadm-scripts/
ls
cd scripts/
ls
vim master.sh 
vim common.sh 
clear
k get pods
clear
k get nodes -o wide
k get pods
exit
